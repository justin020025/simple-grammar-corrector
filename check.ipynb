{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discuss the issue\n",
      "listen to the music\n",
      "study at university\n",
      "stay at home\n",
      "search for more information\n"
     ]
    }
   ],
   "source": [
    "import re, json, operator, sys, urllib, requests, string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log10, log\n",
    "\n",
    "\n",
    "# model path\n",
    "problem_word_path = 'model.txt'\n",
    "\n",
    "# linggle api url\n",
    "NGRAM_API_URI = \"https://{0}.linggle.com/query/\"\n",
    "EXP_API_URI = \"https://{0}.linggle.com/example/\"\n",
    "\n",
    "# 設定最大可分析長度\n",
    "max_len = 5\n",
    "\n",
    "punc = [i for i in string.punctuation]\n",
    "\n",
    "###################\n",
    "# Linggle api \n",
    "###################\n",
    "class Linggle:\n",
    "\tdef __init__(self, ver='www'):\n",
    "\t\tself.ver = ver\n",
    "\tdef __getitem__(self, query):\n",
    "\t\treturn self.search(query)\n",
    "\tdef search(self, query):\n",
    "\t\tquery = query.replace('/', '@')\n",
    "\t\tquery = urllib.parse.quote(query, safe='')\n",
    "\t\treq = requests.get(NGRAM_API_URI.format(self.ver) + query)\n",
    "\t\tresults = req.json()\n",
    "\t\treturn results.get(\"ngrams\", [])\n",
    "\tdef get_example(self, ngram_str):\n",
    "\t\tres = requests.post(EXP_API_URI.format(self.ver), json={'ngram': ngram_str})\n",
    "\t\tif res.status_code == 200:\n",
    "\t\t\tresult = res.json()\n",
    "\t\t\treturn result.get(\"examples\", [])\n",
    "\t\treturn []\n",
    "\n",
    "\n",
    "# 開linggle api\n",
    "ling = Linggle()\n",
    "\n",
    "#####################\n",
    "# Ngram probability #\n",
    "#####################\n",
    "def P(ngram, logN=12., MINCOUNT=40.): \n",
    "    \"Probability of ngram based Web 1T using Linggle API\"\n",
    "    leng = float(len(ngram.split()))\n",
    "    linggle_ngram = ling.search(ngram) # research times\n",
    "    linggle_ngram = linggle_ngram[0][1] if len(linggle_ngram)>0 else 0\n",
    "    return (log(linggle_ngram,10)-12)/pow(leng,1./2.5) if linggle_ngram>0 else (log10(MINCOUNT)-12)\n",
    "\n",
    "##############################################\n",
    "# 編輯(Insert, Delete, Replace)一步之後的結果\n",
    "##############################################\n",
    "def edits1(ngram, model):\n",
    "#\"TODO: handle possible Insert, Delete, Replace edits using data from model\"\n",
    "    words = ngram.split()\n",
    "    store = []\n",
    "    for w_idx, word in enumerate(words):\n",
    "        corr_arr = channel_model(word, model)\n",
    "        for corr in corr_arr:\n",
    "            if corr[0] == 'I': #需insert的情況\n",
    "                in_idx = corr[2]\n",
    "                in_word = corr[1]\n",
    "                in_pos = w_idx + in_idx #插入的位置\n",
    "                if in_idx<0: #做此修正插入位置才正確\n",
    "                    in_pos+=1 \n",
    "                    \n",
    "                if in_pos>=0 and in_pos<=len(words):\n",
    "                    store.append(' '.join(words[:in_pos]+[in_word]+words[in_pos:]))\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            elif corr[0] == 'D': #需delete的情況\n",
    "                d_word = corr[1]\n",
    "                d_idx = corr[2]\n",
    "                d_pos = w_idx + d_idx #del的位置\n",
    "                \n",
    "                if d_pos>=0 and d_pos<len(words) :\n",
    "                    \n",
    "                    new_words = words[:] #使用deep copy\n",
    "                    if new_words[d_pos] == d_word:\n",
    "                        del new_words[d_pos]\n",
    "                        store.append(' '.join(new_words))\n",
    "                    \n",
    "\n",
    "            else:      #需replace的情況\n",
    "                br_word = corr[1]\n",
    "                r_word = corr[2]\n",
    "                if br_word in words:\n",
    "                    br_idx = words.index(br_word)\n",
    "                    new_words = words[:]\n",
    "                    new_words[br_idx] = r_word\n",
    "                    store.append(' '.join(new_words))\n",
    "                \n",
    "    return set(store)\n",
    "                \n",
    "##########################\n",
    "# 編輯兩步之後的結果\n",
    "##########################\n",
    "def edits2(ngram, model): \n",
    "#\"All changes that are two edits away from ngram\"\n",
    "\treturn set(e2 for e1 in edits1(ngram, model) for e2 in edits1(e1, model))\n",
    "\n",
    "#############################\n",
    "# edit 2次之後的 candidates\n",
    "#############################\n",
    "def candidates(ngram, model): \n",
    "\t\"TODO: Generate possible correction\"\n",
    "\treturn set.union({ngram}, edits1(ngram, model),edits2(ngram, model))\n",
    "\n",
    "###############################\n",
    "# 找最好的編輯\n",
    "###############################\n",
    "def correction(ngram, model): \n",
    "#\"TODO: Return most probable grammatical error correction for ngram.\"\n",
    "    #print(candidates(ngram, model))\n",
    "    #print([P(g) for g in candidates(ngram, model)])\n",
    "    return max(candidates(ngram, model), key=P)\n",
    "\n",
    "################################\n",
    "# 找出problem word有沒有在model裡\n",
    "################################\n",
    "def channel_model(problem_word, model):\n",
    "\treturn model[problem_word] if(problem_word in model) else []\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# 把model json檔獨進來\n",
    "##############################\n",
    "def read_problem_word(path):\n",
    "\twith open(path, 'r') as f:\n",
    "\t\tmodel = json.load(f)\n",
    "\treturn model\n",
    "\n",
    "with open('model.txt', 'r') as f:\n",
    "    model = json.load(f)\n",
    "    \n",
    "with open('input.txt','r') as inputf, open('output.txt', 'w') as outputf:\n",
    "    lines = inputf.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        corr_line = correction(line[:-1], model) #line[:-1]去除最後面的'\\n'\n",
    "        outputf.write(corr_line + '\\n')\n",
    "        print(corr_line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
